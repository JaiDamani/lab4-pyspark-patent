{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43103119-8c45-4aca-b05d-e1785a82c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PatentJoinDataFrame\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3583c4d-1b28-4a25-998f-8e1a7a4b575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patent_path = \"apat63_99.txt\"   \n",
    "citations_path = \"cite75_99.txt\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66d83565-9d3b-4177-a157-50831638d66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patents columns: ['_c0', '_c1', '_c2', '_c3', '_c4', '_c5', '_c6', '_c7', '_c8', '_c9', '_c10', '_c11', '_c12', '_c13', '_c14', '_c15', '_c16', '_c17', '_c18', '_c19', '_c20', '_c21', '_c22']\n",
      "+-------+-----+-----+-------+-------+-------+--------+-------+------+------+----+------+-----+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|_c0    |_c1  |_c2  |_c3    |_c4    |_c5    |_c6     |_c7    |_c8   |_c9   |_c10|_c11  |_c12 |_c13    |_c14    |_c15   |_c16    |_c17    |_c18    |_c19    |_c20    |_c21    |_c22    |\n",
      "+-------+-----+-----+-------+-------+-------+--------+-------+------+------+----+------+-----+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|PATENT |GYEAR|GDATE|APPYEAR|COUNTRY|POSTATE|ASSIGNEE|ASSCODE|CLAIMS|NCLASS|CAT |SUBCAT|CMADE|CRECEIVE|RATIOCIT|GENERAL|ORIGINAL|FWDAPLAG|BCKGTLAG|SELFCTUB|SELFCTLB|SECDUPBD|SECDLWBD|\n",
      "|3070801|1963 |1096 |NULL   |BE     |NULL   |NULL    |1      |NULL  |269   |6   |69    |NULL |1       |NULL    |0      |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |\n",
      "|3070802|1963 |1096 |NULL   |US     |TX     |NULL    |1      |NULL  |2     |6   |63    |NULL |0       |NULL    |NULL   |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |\n",
      "|3070803|1963 |1096 |NULL   |US     |IL     |NULL    |1      |NULL  |2     |6   |63    |NULL |9       |NULL    |0.3704 |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |\n",
      "|3070804|1963 |1096 |NULL   |US     |OH     |NULL    |1      |NULL  |2     |6   |63    |NULL |3       |NULL    |0.6667 |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |\n",
      "+-------+-----+-----+-------+-------+-------+--------+-------+------+------+----+------+-----+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patents_df = spark.read.csv(patent_path, header=False, quote='\"', multiLine=False).cache()\n",
    "\n",
    "print(\"Patents columns:\", patents_df.columns)\n",
    "patents_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8015924d-a514-4f50-a0a5-933010b036a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+\n",
      "|patent_id|country|state  |\n",
      "+---------+-------+-------+\n",
      "|NULL     |COUNTRY|POSTATE|\n",
      "|3070801  |BE     |NULL   |\n",
      "|3070802  |US     |TX     |\n",
      "|3070803  |US     |IL     |\n",
      "|3070804  |US     |OH     |\n",
      "|3070805  |US     |CA     |\n",
      "|3070806  |US     |PA     |\n",
      "|3070807  |US     |OH     |\n",
      "|3070808  |US     |IA     |\n",
      "|3070809  |US     |AZ     |\n",
      "+---------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pat_meta = patents_df.select(\n",
    "    F.col(\"_c0\").alias(\"patent_id\").cast(IntegerType()),\n",
    "    F.col(\"_c4\").alias(\"country\"),\n",
    "    F.col(\"_c5\").alias(\"state\")\n",
    ").withColumn(\"state\", F.when(F.col(\"state\") == \"\", None).otherwise(F.col(\"state\"))) \\\n",
    " .cache()\n",
    "\n",
    "pat_meta.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a76509f-c327-4e24-8f2b-46bcafd6575e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|citing|cited|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "citations_df = spark.read.text(citations_path).select(\n",
    "    F.split(F.col(\"value\"), \"\\\\s+\").alias(\"parts\")\n",
    ").filter(F.size(\"parts\") == 2).select(\n",
    "    F.col(\"parts\").getItem(0).cast(IntegerType()).alias(\"citing\"),\n",
    "    F.col(\"parts\").getItem(1).cast(IntegerType()).alias(\"cited\")\n",
    ").cache()\n",
    "\n",
    "citations_df.show(6, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69dda5a5-54b5-44e9-9606-a3407ed4c1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----------+------------+\n",
      "|citing|cited|cited_state|citing_state|\n",
      "+------+-----+-----------+------------+\n",
      "+------+-----+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cited_join = citations_df.join(\n",
    "    pat_meta.select(F.col(\"patent_id\").alias(\"cited\"), F.col(\"state\").alias(\"cited_state\")),\n",
    "    on=\"cited\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "cited_with_states = cited_join.join(\n",
    "    pat_meta.select(F.col(\"patent_id\").alias(\"citing\"), F.col(\"state\").alias(\"citing_state\")),\n",
    "    on=\"citing\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "cited_with_states.select(\"citing\", \"cited\", \"cited_state\", \"citing_state\").show(8, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24e361aa-3001-4114-ab38-7a8753386338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|citing|same_state_citations|\n",
      "+------+--------------------+\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "same_state_counts = cited_with_states \\\n",
    "    .filter((F.col(\"cited_state\").isNotNull()) & (F.col(\"citing_state\").isNotNull())) \\\n",
    "    .withColumn(\"same\", F.when(F.col(\"cited_state\") == F.col(\"citing_state\"), 1).otherwise(0)) \\\n",
    "    .groupBy(\"citing\") \\\n",
    "    .agg(F.sum(\"same\").cast(IntegerType()).alias(\"same_state_citations\")) \\\n",
    "    .cache()\n",
    "\n",
    "same_state_counts.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4ec500b-6168-4928-927f-7fec29a7a728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+--------------------+\n",
      "|_c0    |_c4    |_c5    |same_state_citations|\n",
      "+-------+-------+-------+--------------------+\n",
      "|PATENT |COUNTRY|POSTATE|NULL                |\n",
      "|3070801|BE     |NULL   |NULL                |\n",
      "|3070802|US     |TX     |0                   |\n",
      "|3070803|US     |IL     |0                   |\n",
      "|3070804|US     |OH     |0                   |\n",
      "|3070805|US     |CA     |0                   |\n",
      "|3070806|US     |PA     |0                   |\n",
      "|3070807|US     |OH     |0                   |\n",
      "|3070808|US     |IA     |0                   |\n",
      "|3070809|US     |AZ     |0                   |\n",
      "|3070810|US     |IL     |0                   |\n",
      "|3070811|US     |CA     |0                   |\n",
      "+-------+-------+-------+--------------------+\n",
      "only showing top 12 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patents_full = patents_df.withColumn(\"patent_id\", F.col(\"_c0\").cast(IntegerType()))\n",
    "\n",
    "patents_aug = patents_full.join(\n",
    "    same_state_counts,\n",
    "    patents_full.patent_id == same_state_counts.citing,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "patents_aug = patents_aug.withColumn(\n",
    "    \"same_state_citations\",\n",
    "    F.when(\n",
    "        (F.col(\"_c4\") == \"US\") & (F.col(\"_c5\").isNotNull()) & (F.col(\"_c5\") != \"\"),\n",
    "        F.coalesce(F.col(\"same_state_citations\"), F.lit(0))\n",
    "    ).otherwise(F.lit(None))\n",
    ")\n",
    "\n",
    "patents_aug.select(\"_c0\", \"_c4\", \"_c5\", \"same_state_citations\").show(12, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52aeb57e-bfde-498c-b8fa-e8ee620cd791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|patent_id|same_state_citations|\n",
      "+---------+--------------------+\n",
      "|4649756  |0                   |\n",
      "|3070802  |0                   |\n",
      "|4649759  |0                   |\n",
      "|3070803  |0                   |\n",
      "|4649760  |0                   |\n",
      "|3070804  |0                   |\n",
      "|4649766  |0                   |\n",
      "|3070805  |0                   |\n",
      "|4649767  |0                   |\n",
      "|3070806  |0                   |\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top10 = patents_aug \\\n",
    "    .select(F.col(\"_c0\").cast(IntegerType()).alias(\"patent_id\"), \"same_state_citations\") \\\n",
    "    .filter(F.col(\"same_state_citations\").isNotNull()) \\\n",
    "    .orderBy(F.col(\"same_state_citations\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "top10.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "290eb226-9b43-47d9-823b-fd5966ccc054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|augmented_line                                                                                                                                                                      |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|PATENT,GYEAR,GDATE,APPYEAR,COUNTRY,POSTATE,ASSIGNEE,ASSCODE,CLAIMS,NCLASS,CAT,SUBCAT,CMADE,CRECEIVE,RATIOCIT,GENERAL,ORIGINAL,FWDAPLAG,BCKGTLAG,SELFCTUB,SELFCTLB,SECDUPBD,SECDLWBD,|\n",
      "|3070801,1963,1096,,BE,,,1,,269,6,69,,1,,0,,,,,,,,                                                                                                                                   |\n",
      "|3070802,1963,1096,,US,TX,,1,,2,6,63,,0,,,,,,,,,,0                                                                                                                                   |\n",
      "|3070803,1963,1096,,US,IL,,1,,2,6,63,,9,,0.3704,,,,,,,,0                                                                                                                             |\n",
      "|3070804,1963,1096,,US,OH,,1,,2,6,63,,3,,0.6667,,,,,,,,0                                                                                                                             |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "original_cols = patents_df.columns  # _c0, _c1, ... as strings\n",
    "cols_to_concat = [F.coalesce(F.col(c).cast(StringType()), F.lit('')) for c in original_cols] \\\n",
    "                 + [F.coalesce(F.col(\"same_state_citations\").cast(StringType()), F.lit(''))]\n",
    "\n",
    "patents_with_augmented_line = patents_aug.withColumn(\"augmented_line\", F.concat_ws(\",\", *cols_to_concat))\n",
    "patents_with_augmented_line.select(\"augmented_line\").show(5, truncate=False)\n",
    "\n",
    "\n",
    "patents_with_augmented_line.select(\"augmented_line\").write.text(\"patents_augmented_out\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7df6450-ce49-40bd-9394-ba31b9ba96de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PatentJoinLab\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get the SparkContext from the SparkSession\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99623598-2f84-46f4-98d5-e9dcbb2cd4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD solution: run in the same Spark session, using sc\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19af79f7-6df3-4573-804a-050c0d9dc536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD Cell 1: helper CSV parsing inside mapPartitions\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "def parse_csv_partition(lines):\n",
    "    # csv.reader expects an iterable of lines -- we can feed it directly\n",
    "    for row in csv.reader(lines):\n",
    "        # yield list of fields\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75cf617f-df98-4972-8e2f-e7645222fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD Cell 2: read patents lines and parse\n",
    "patent_lines = sc.textFile(\"apat63_99.txt\")\n",
    "patent_parsed = patent_lines.mapPartitions(parse_csv_partition).cache()\n",
    "# patent_parsed: each element is a list of fields, e.g. [\"6009554\",\"1999\",\"14606\",\"1997\",\"US\",\"NY\", ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52eda9c2-52cd-44d2-a34f-eee8adcc1e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD Cell 3: build (patent_id, state) and also (patent_id, fields_list) for reassembly\n",
    "def extract_patent_state(fields):\n",
    "    try:\n",
    "        pid = int(fields[0])\n",
    "    except Exception:\n",
    "        return None\n",
    "    # state at index 5 if present\n",
    "    state = None\n",
    "    if len(fields) > 5:\n",
    "        st = fields[5]\n",
    "        if st != '':\n",
    "            state = st\n",
    "    return (pid, state)\n",
    "\n",
    "patent_states = patent_parsed.map(lambda fields: extract_patent_state(fields)).filter(lambda x: x is not None).cache()\n",
    "# patent_states: (pid, state or None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb41d75d-24dd-4cd8-9bcb-5992e4b98d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD Cell 4: read citations and map to (citing, cited)\n",
    "cit_rdd = sc.textFile(\"cite75_99.txt\") \\\n",
    "    .map(lambda l: l.strip().split()) \\\n",
    "    .filter(lambda parts: len(parts) == 2) \\\n",
    "    .map(lambda parts: (int(parts[0]), int(parts[1]))) \\\n",
    "    .cache()\n",
    "# cit_rdd: (citing, cited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1533350f-5e8a-45a4-857d-a3fcf7463705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD Cell 5: Get cited_state per (citing, cited)\n",
    "# First map citations keyed by cited: (cited, citing)\n",
    "cit_by_cited = cit_rdd.map(lambda citing_cited: (citing_cited[1], citing_cited[0]))  # (cited, citing)\n",
    "\n",
    "# join with patent_states (keyed by patent_id)\n",
    "# ensure patent_states keyed by pid\n",
    "pat_states_kv = patent_states  # (pid, state)\n",
    "# join on cited -> (cited, (citing, cited_state))\n",
    "joined_cited = cit_by_cited.join(pat_states_kv)  # join uses RDD.join, result only where state exists in pat_states_kv (but pat_states_kv contains even None states)\n",
    "# NOTE: if patent_states has entries with state==None, join still produces (cited, (citing, None)), we keep that so we can filter later\n",
    "\n",
    "# map to (citing, cited_state) for each citation\n",
    "citing_to_cited_state = joined_cited.map(lambda cid_citingState: (cid_citingState[1][0], cid_citingState[1][1]))  # (citing, cited_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eed3421c-38f2-4081-9f22-1dc376e5a3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[42] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD Cell 6: bring in citing_state by joining citing_to_cited_state with patent_states keyed by citing\n",
    "# First ensure patent_states keyed by patent id - it's already (pid, state)\n",
    "# join on citing as key\n",
    "joined_for_citing = citing_to_cited_state.join(pat_states_kv)  # key=citing -> (citing, (cited_state, citing_state))\n",
    "\n",
    "# Now filter where both states are non-null and compare equality; produce (citing, 1) for same-state\n",
    "same_pairs = joined_for_citing.filter(lambda kv: (kv[1][0] is not None) and (kv[1][1] is not None) and (kv[1][0] == kv[1][1])) \\\n",
    "    .map(lambda kv: (kv[0], 1))\n",
    "\n",
    "# reduceByKey to count same-state cites\n",
    "same_state_counts = same_pairs.reduceByKey(lambda a,b: a+b)  # (citing, count)\n",
    "same_state_counts.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26388c52-6617-4c17-93e5-25234a21de04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3070801', '\"\"'),\n",
       " ('3070802', '\"TX\"'),\n",
       " ('3070803', '\"IL\"'),\n",
       " ('3070804', '\"OH\"'),\n",
       " ('3070805', '\"CA\"')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load patents\n",
    "lines_rdd = sc.textFile(\"apat63_99.txt\")\n",
    "\n",
    "# Remove header\n",
    "header = lines_rdd.first()\n",
    "data_rdd = lines_rdd.filter(lambda row: row != header)\n",
    "\n",
    "# Split into columns\n",
    "parsed_rdd = data_rdd.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Extract (patent_id, state)\n",
    "patent_states = parsed_rdd.map(lambda cols: (cols[0], cols[5]))\n",
    "\n",
    "# Check first few\n",
    "patent_states.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ec663da-71fc-4319-94da-2f799b45aaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"PATENT\",\"GYEAR\",\"GDATE\",\"APPYEAR\",\"COUNTRY\",\"POSTATE\",\"ASSIGNEE\",\"ASSCODE\",\"CLAIMS\",\"NCLASS\",\"CAT\",\"SUBCAT\",\"CMADE\",\"CRECEIVE\",\"RATIOCIT\",\"GENERAL\",\"ORIGINAL\",\"FWDAPLAG\",\"BCKGTLAG\",\"SELFCTUB\",\"SELFCTLB\",\"SECDUPBD\",\"SECDLWBD\"\n",
      "3070801,1963,1096,,\"BE\",\"\",,1,,269,6,69,,1,,0,,,,,,,\n",
      "3070802,1963,1096,,\"US\",\"TX\",,1,,2,6,63,,0,,,,,,,,,\n",
      "3070803,1963,1096,,\"US\",\"IL\",,1,,2,6,63,,9,,0.3704,,,,,,,\n",
      "3070804,1963,1096,,\"US\",\"OH\",,1,,2,6,63,,3,,0.6667,,,,,,,\n"
     ]
    }
   ],
   "source": [
    "lines_rdd = sc.textFile(\"apat63_99.txt\")\n",
    "\n",
    "# Grab header\n",
    "header = lines_rdd.first()\n",
    "\n",
    "# Filter it out\n",
    "data_rdd = lines_rdd.filter(lambda row: row != header)\n",
    "\n",
    "# Now parse safely\n",
    "parsed_rdd = data_rdd.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04a0bc7f-da32-49e3-a446-0155f1ab8a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('956203', '3858241'),\n",
       " ('1324234', '3858241'),\n",
       " ('3398406', '3858241'),\n",
       " ('3557384', '3858241'),\n",
       " ('3634889', '3858241')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load citations\n",
    "citations_rdd = sc.textFile(\"cite75_99.txt\")\n",
    "\n",
    "# Remove header\n",
    "cite_header = citations_rdd.first()\n",
    "data_citations_rdd = citations_rdd.filter(lambda row: row != cite_header)\n",
    "\n",
    "# Split\n",
    "parsed_citations_rdd = data_citations_rdd.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Extract (cited, citing)\n",
    "cit_by_cited = parsed_citations_rdd.map(lambda cols: (cols[1], cols[0]))\n",
    "\n",
    "# Check\n",
    "cit_by_cited.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29db8939-32a6-4ce9-a98f-ff3368e21274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3858315', '\"MO\"'),\n",
       " ('3861040', '\"MO\"'),\n",
       " ('3863340', '\"MO\"'),\n",
       " ('3872592', '\"MO\"'),\n",
       " ('3889369', '\"MO\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (cited, (citing, cited_state))\n",
    "joined_cited = cit_by_cited.join(patent_states)\n",
    "\n",
    "# (citing, cited_state)\n",
    "citing_to_cited_state = joined_cited.map(lambda x: (x[1][0], x[1][1]))\n",
    "\n",
    "citing_to_cited_state.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0eaa4f9e-38a3-4e77-ad4d-38c0ac60adff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4125939', ('\"MO\"', '\"NY\"')),\n",
       " ('4125939', ('\"CT\"', '\"NY\"')),\n",
       " ('4409735', ('\"MO\"', '\"CT\"')),\n",
       " ('4409735', ('\"MA\"', '\"CT\"')),\n",
       " ('4409735', ('\"NY\"', '\"CT\"'))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (citing, (cited_state, citing_state))\n",
    "joined_with_states = citing_to_cited_state.join(patent_states)\n",
    "\n",
    "joined_with_states.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66199300-24af-43c8-ac03-fb55f8733ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('\"\"', '\"\"'), 2382043),\n",
       " (('\"\"', '\"NY\"'), 252405),\n",
       " (('\"MA\"', '\"AZ\"'), 3843),\n",
       " (('\"NC\"', '\"NC\"'), 12975),\n",
       " (('\"GA\"', '\"OH\"'), 6168),\n",
       " (('\"NJ\"', '\"LA\"'), 2439),\n",
       " (('\"UT\"', '\"WI\"'), 933),\n",
       " (('\"MO\"', '\"KS\"'), 864),\n",
       " (('\"CA\"', '\"WV\"'), 1403),\n",
       " (('\"LA\"', '\"\"'), 12816),\n",
       " (('\"SC\"', '\"NY\"'), 3611),\n",
       " (('\"SC\"', '\"NJ\"'), 2889),\n",
       " (('\"KY\"', '\"CO\"'), 416),\n",
       " (('\"MN\"', '\"TX\"'), 9103),\n",
       " (('\"TX\"', '\"OH\"'), 15006),\n",
       " (('\"CT\"', '\"OH\"'), 10489),\n",
       " (('\"TX\"', '\"MA\"'), 16788),\n",
       " (('\"TX\"', '\"IL\"'), 20830),\n",
       " (('\"WI\"', '\"CA\"'), 16353),\n",
       " (('\"CA\"', '\"AZ\"'), 17059)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map to ((citing_state, cited_state), 1)\n",
    "state_pairs = joined_with_states.map(lambda x: ((x[1][1], x[1][0]), 1))\n",
    "\n",
    "# Reduce\n",
    "state_pair_counts = state_pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Show results\n",
    "state_pair_counts.take(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d794f9ab-237d-41d2-933b-87e39c743d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to folder\n",
    "state_pair_counts.saveAsTextFile(\"state_citation_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be4bcafd-2f02-4dc4-947e-058f0e20bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_pair_counts.coalesce(1).saveAsTextFile(\"state_citation_counts_single\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3f7dd71-04f9-4fae-ac32-fa86b92ec23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\"\"', '\"\"') 2382043\n",
      "('\"\"', '\"NY\"') 252405\n",
      "('\"MA\"', '\"AZ\"') 3843\n",
      "('\"NC\"', '\"NC\"') 12975\n",
      "('\"GA\"', '\"OH\"') 6168\n",
      "('\"NJ\"', '\"LA\"') 2439\n",
      "('\"UT\"', '\"WI\"') 933\n",
      "('\"MO\"', '\"KS\"') 864\n",
      "('\"CA\"', '\"WV\"') 1403\n",
      "('\"LA\"', '\"\"') 12816\n",
      "('\"SC\"', '\"NY\"') 3611\n",
      "('\"SC\"', '\"NJ\"') 2889\n",
      "('\"KY\"', '\"CO\"') 416\n",
      "('\"MN\"', '\"TX\"') 9103\n",
      "('\"TX\"', '\"OH\"') 15006\n",
      "('\"CT\"', '\"OH\"') 10489\n",
      "('\"TX\"', '\"MA\"') 16788\n",
      "('\"TX\"', '\"IL\"') 20830\n",
      "('\"WI\"', '\"CA\"') 16353\n",
      "('\"CA\"', '\"AZ\"') 17059\n"
     ]
    }
   ],
   "source": [
    "for pair, count in state_pair_counts.take(20):\n",
    "    print(pair, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e12fbe27-9b94-4b6b-917d-e929557bb4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe version 2\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start SparkSession (if not already started)\n",
    "spark = SparkSession.builder.appName(\"PatentCitationsDF\").getOrCreate()\n",
    "\n",
    "# Load patents file\n",
    "patents_df = spark.read.csv(\"apat63_99.txt\", header=True, inferSchema=True)\n",
    "\n",
    "# Load citations file\n",
    "citations_df = spark.read.csv(\"cite75_99.txt\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc5d2e77-0478-4076-80cd-08fd3a72f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "patents_states = patents_df.select(\"PATENT\", \"POSTATE\")\n",
    "citations = citations_df.select(\"CITING\", \"CITED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "853bf44e-61f9-41f6-a9ed-5f0d9f86189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join citations with patents (for citing side)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "citing_with_state = citations.join(\n",
    "    patents_states.withColumnRenamed(\"PATENT\", \"CITING_PATENT\")\n",
    "                  .withColumnRenamed(\"POSTATE\", \"CITING_STATE\"),\n",
    "    citations[\"CITING\"] == F.col(\"CITING_PATENT\"),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Join again for cited side\n",
    "joined_df = citing_with_state.join(\n",
    "    patents_states.withColumnRenamed(\"PATENT\", \"CITED_PATENT\")\n",
    "                  .withColumnRenamed(\"POSTATE\", \"CITED_STATE\"),\n",
    "    citing_with_state[\"CITED\"] == F.col(\"CITED_PATENT\"),\n",
    "    \"inner\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06f5ef83-0d43-43ce-a957-620ceb933ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_pair_counts_df = (\n",
    "    joined_df.groupBy(\"CITING_STATE\", \"CITED_STATE\")\n",
    "             .count()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "755fa1ba-c758-44aa-9e68-5df9c70ed8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----+\n",
      "|CITING_STATE|CITED_STATE|count|\n",
      "+------------+-----------+-----+\n",
      "|          SC|         PA| 2987|\n",
      "|          MN|         IL|14046|\n",
      "|          KS|         MN|  962|\n",
      "|          CO|         ID|  543|\n",
      "|          DC|         MA|  258|\n",
      "|          NC|         OR|  874|\n",
      "|          IL|         ND|  417|\n",
      "|          KS|         OK|  558|\n",
      "|          IA|         CO|  703|\n",
      "|          NE|         AZ|  165|\n",
      "+------------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_pair_counts_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9d38009-ac9f-480c-b7bf-eca79a39fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_pair_counts_df.write.csv(\"state_citation_counts_df\", header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1bd7226a-66e0-438a-8950-baef225ac8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = {\n",
    "    (row[\"CITING_STATE\"], row[\"CITED_STATE\"]): row[\"count\"]\n",
    "    for row in state_pair_counts_df.collect()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3660075d-b9ef-4f49-824e-c272d03a4946",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_results = dict(state_pair_counts.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb3de0f0-76aa-4164-8ce8-26d8faade593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_state(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    s = s.strip().upper().replace('\"', '')\n",
    "    return s if s else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95a41a02-24e0-4349-ad9c-b82f60eae068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do RDD and DataFrame outputs match? False\n"
     ]
    }
   ],
   "source": [
    "print(\"Do RDD and DataFrame outputs match?\", df_results == rdd_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fbc4d3f4-0747-454e-9bd9-cd04f67b3abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"file.csv\").map(lambda line: line.split(\",\"))\n",
    "pairs_rdd = rdd.map(lambda row: ((row[0], row[1]), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3606518e-3efc-48f7-9960-a0dce42371ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_rdd = rdd.map(lambda row: (\n",
    "    (normalize_state(row[0]), normalize_state(row[1])), 1\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "181cf2ff-0bf3-4c3b-a750-8eecbd24fdba",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o592.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/jovyan/lab4-pyspark-patent/file.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat jdk.internal.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.io.IOException: Input path does not exist: file:/home/jovyan/lab4-pyspark-patent/file.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 24 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m counts_rdd \u001b[38;5;241m=\u001b[39m \u001b[43mpairs_rdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduceByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:3552\u001b[0m, in \u001b[0;36mRDD.reduceByKey\u001b[0;34m(self, func, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   3505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduceByKey\u001b[39m(\n\u001b[1;32m   3506\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Tuple[K, V]]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3507\u001b[0m     func: Callable[[V, V], V],\n\u001b[1;32m   3508\u001b[0m     numPartitions: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3509\u001b[0m     partitionFunc: Callable[[K], \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m portable_hash,\n\u001b[1;32m   3510\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Tuple[K, V]]\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3511\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3512\u001b[0m \u001b[38;5;124;03m    Merge the values for each key using an associative and commutative reduce function.\u001b[39;00m\n\u001b[1;32m   3513\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3550\u001b[0m \u001b[38;5;124;03m    [('a', 2), ('b', 1)]\u001b[39;00m\n\u001b[1;32m   3551\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombineByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumPartitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitionFunc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:3975\u001b[0m, in \u001b[0;36mRDD.combineByKey\u001b[0;34m(self, createCombiner, mergeValue, mergeCombiners, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   3913\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3914\u001b[0m \u001b[38;5;124;03mGeneric function to combine the elements for each key using a custom\u001b[39;00m\n\u001b[1;32m   3915\u001b[0m \u001b[38;5;124;03mset of aggregation functions.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3972\u001b[0m \u001b[38;5;124;03m[('a', [1, 2]), ('b', [1])]\u001b[39;00m\n\u001b[1;32m   3973\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numPartitions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3975\u001b[0m     numPartitions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_defaultReducePartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3977\u001b[0m serializer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39mserializer\n\u001b[1;32m   3978\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_limit()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:4867\u001b[0m, in \u001b[0;36mRDD._defaultReducePartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39mdefaultParallelism\n\u001b[1;32m   4866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetNumPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:5453\u001b[0m, in \u001b[0;36mPipelinedRDD.getNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetNumPartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m-> 5453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o592.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/jovyan/lab4-pyspark-patent/file.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat jdk.internal.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.io.IOException: Input path does not exist: file:/home/jovyan/lab4-pyspark-patent/file.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 24 more\n"
     ]
    }
   ],
   "source": [
    "counts_rdd = pairs_rdd.reduceByKey(lambda a, b: a + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87838aa-cc10-43b3-a728-0b67ab8eadec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
